<!DOCTYPE html>

<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141678413-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141678413-1');
</script>
<link rel="stylesheet" type="text/css" href="style.css">
<title>Soham De</title>
<script src="https://code.jquery.com/jquery-1.10.2.js"></script>
<script> 
$(function(){
	$("#visible").click(function() {
	    $('#invisible').toggleClass("show");
    });
});
</script> 
<script>
    function unhide(divID) {
            var item = document.getElementById(divID);
            if (item) {
                item.className=(item.className=='hidden')?'unhidden':'hidden';
            }
    }
</script>

<style>
  .hide{display:none;}
  .show{display:block;}
</style>
</head>

<body>
<img src="profilepic.jpg" style="float:right;height:240px;border-radius:90%;" hspace="20">

<h3>SOHAM DE</h3>
Research Scientist <br>
<a target="_blank" href="http://deepmind.com">DeepMind</a><br/>
London, UK<br/><br/>
<b>Contact Address:</b><br/>
R7, 14-18 Handyside St,</br>
London, UK, N1C 4DN<br/><br/>
<a target="_blank" href="mailto: sohamde.ml@gmail.com">Email</a> / <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=lHf55pF3KVQC&view_op=list_works">Scholar</a> / <a target="_blank" href="https://twitter.com/sohamde_">Twitter</a>
<!--<a target="_blank" href="cv.pdf">CV</a><br/>-->

<br/><br/>
I am a research scientist at <a href="https://deepmind.com/" target="_blank">DeepMind</a> in London, where I work on better understanding and improving large-scale deep learning.
I currently focus on topics in optimization and initialization, and more recently have been working on privacy-preserving machine learning. Feel free to drop me a line if you want to chat about any of these topics.
<br/><br/>
	
I completed my PhD in 2018 at the <a href="https://www.cs.umd.edu/" target="_blank">University of Maryland</a>, where I was advised by <a href="https://www.cs.umd.edu/~nau/" target="_blank">Dana Nau</a> and <a href="http://www.cs.umd.edu/~tomg/" target="_blank">Tom Goldstein</a>.
During my PhD, I worked on fast stochastic optimization algorithms for machine learning problems.
I also worked on game-theoretic models of the evolution of human behavioral norms collaborating with <a href="https://www.michelegelfand.com/" target="_blank">Michele Gelfand</a>.
I completed my undergraduate degree from <a href="http://www.jaduniv.edu.in/" target="_blank">Jadavpur University</a>, Kolkata, India in 2013.
I have previously interned at <a href="https://www.ttic.edu/" target="_blank">Toyota Technological Institute at Chicago (TTIC)</a>, <a href="http://research.ibm.com/labs/almaden/" target="_blank">IBM Research Almaden</a> and <a href="https://deepmind.com/" target="_blank">DeepMind</a>.

<br/><br/>
<h3>RECENT TALKS</h3>
<ul>
	<li>
		<span class="title">High-Accuracy Differentially Private Image Classification</span>: <a href="docs/dpsgd-talk.pdf" target="_blank">slides</a><br/>
		At Google Brain, University College London, University of Maryland, Security & Privacy in ML Seminar, Oracle Labs and Indian Institute of Technology Kharagpur. (2022)
		<br/><br/>
		
	<li>		
		<span class="title">Normalizer-Free Networks</span>: <a href="docs/nfnets-talk.pdf" target="_blank">slides</a><br/>
		At Google Brain, University College London and Swiss Federal Institute of Technology Lausanne (EPFL). (2021)
</ul>

<h3>SELECTED PAPERS</h3>
Full List: <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=lHf55pF3KVQC&view_op=list_works&sortby=pubdate">Google Scholar</a>
<br/><br/>

<ul>
	<li>
	<span class="title">Unlocking High-Accuracy Differentially Private Image Classification through Scale</span><br/> 
	<b>Soham De</b>, Leonard Berrada, Jamie Hayes, Samuel L Smith, Borja Balle<br/>
	Preprint<br/>
	<a href="https://arxiv.org/pdf/2204.13650.pdf" target="_blank">paper</a> /
	<a href="https://github.com/deepmind/jax_privacy" target="_blank">code</a> /
	<a href="javascript:unhide('dpsgd22bib');">bibtex</a> /
	<a href="javascript:unhide('dpsgd22tldr');">tl;dr</a>
	<div id="dpsgd22bib" class="hidden"><div class="bibstyle"><br/>
@article{de2022unlocking, <br/>
  title={Unlocking high-accuracy differentially private image classification through scale}, <br/>
  author={De, Soham and Berrada, Leonard and Hayes, Jamie and Smith, Samuel L and Balle, Borja}, <br/>
  journal={arXiv preprint arXiv:2204.13650}, <br/>
  year={2022} <br/>
} <br/>
	</div></div>
	<div id="dpsgd22tldr" class="hidden"><br/>
Machine learning models that were trained under standard training pipelines can leak the data it was trained on. This presents privacy risks when the training data contains sensitive information. Differential privacy is a technology that can be deployed at training time to mitigate these privacy risks, but they often incur significant reduction in model performance. In this work, we make substantial progress towards unlocking high-accuracy training of image classification models under differential privacy.
	</div>
	<br/><br/>
	
	<li>
	<span class="title">High-Performance Large-Scale Image Recognition Without Normalization</span><br/> 
	Andrew Brock, <b>Soham De</b>, Sam Smith, Karen Simonyan<br/>
	ICML 2021<br/>
	<a href="https://arxiv.org/pdf/2102.06171.pdf" target="_blank">paper</a> /
	<a href="https://github.com/deepmind/deepmind-research/tree/master/nfnets" target="_blank">code</a> /
	<a href="javascript:unhide('nfnet21bib');">bibtex</a> /
	<a href="javascript:unhide('nfnet21tldr');">tl;dr</a>
	<div id="nfnet21bib" class="hidden"><div class="bibstyle"><br/>
@article{brock2021high, <br/>
  title={High-Performance Large-Scale Image Recognition Without Normalization}, <br/>
  author={Brock, Andrew and De, Soham and Smith, Samuel L and Simonyan, Karen}, <br/>
  journal={arXiv preprint arXiv:2102.06171}, <br/>
  year={2021} <br/>
} <br/>
	</div></div>
	<div id="nfnet21tldr" class="hidden"><br/>
BatchNorm is a key component of most vision models, but it has many undesirable properties stemming from its dependence on the batch size and the fact that it introduces interactions between examples. We propose NFNets, a new family of ResNet models, that achieve state-of-the-art performance on ImageNet across a range of training latencies and is up to 8.7 times faster than EfficientNets, without any normalization layers.
	</div>
	<br/><br/>
		
	<li>
	<span class="title">On the Origin of Implicit Regularization in Stochastic Gradient Descent</span><br/> 
	Sam Smith, Benoit Dherin, David Barrett, <b>Soham De</b><br/>
	ICLR 2021<br/>
	<a href="https://arxiv.org/pdf/2101.12176.pdf" target="_blank">paper</a> /
	<a href="javascript:unhide('beasgd21bib');">bibtex</a> /
	<a href="javascript:unhide('beasgd21tldr');">tl;dr</a>
	<div id="beasgd21bib" class="hidden"><div class="bibstyle"><br/>
@inproceedings{smith2021origin, <br/>
  title={On the Origin of Implicit Regularization in Stochastic Gradient Descent}, <br/>
  author={Smith, Samuel L and Dherin, Benoit and Barrett, David GT and De, Soham}, <br/>
  booktitle={9th International Conference on Learning Representations, {ICLR}}, <br/>
  year={2021} <br/>
} <br/>
	</div></div>
	<div id="beasgd21tldr" class="hidden"><br/>
We show that, when using small finite learning rates, the path taken by Random Shuffling SGD coincides with the path taken by gradient flow when minimizing a modified loss function. This modified loss function contains an implicit source of regularization which enhances the test accuracies achieved by deep networks on standard benchmarks.
	</div>
	<br/><br/>
	
<!--	<li>
	<span class="title">Characterizing Signal Propagation to Close the Performance Gap in Unnormalized ResNets</span><br/> 
	Andrew Brock, <b>Soham De</b>, Sam Smith<br/>
	ICLR 2021<br/>
	<a href="https://arxiv.org/pdf/2101.08692.pdf" target="_blank">paper</a> /
	<a href="javascript:unhide('nfresnet21bib');">bibtex</a> /
	<a href="javascript:unhide('nfresnet21tldr');">tl;dr</a>
	<div id="nfresnet21bib" class="hidden"><div class="bibstyle"><br/>
@inproceedings{brock2021characterizing, <br/>
  title={Characterizing signal propagation to close the performance gap in unnormalized ResNets}, <br/>
  author={Brock, Andrew and De, Soham and Smith, Samuel L}, <br/>
  booktitle={9th International Conference on Learning Representations, {ICLR}}, <br/>
  year={2021} <br/>
} <br/>
	</div></div>
	<div id="nfresnet21tldr" class="hidden"><br/>
We propose signal propagation plots (SPPs) to characterize signal propagation on the forward pass in deep networks. Using SPPs, we show that a key benefit of BatchNorm is that it corrects for a mean-shift in the hidden activations that can arise with ReLU activations. We propose a modified version of Weight Standardization that can correct for this, and train normalizer-free ResNets to high accuracies.
	</div>
	<br/><br/>
-->

	<li>
	<span class="title">Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks</span><br/> 
	<b>Soham De</b>, Sam Smith<br/>
	NeurIPS 2020<br/>
	<a href="https://arxiv.org/pdf/2002.10444.pdf" target="_blank">paper</a> /
	<a href="javascript:unhide('bn20bib');">bibtex</a> /
	<a href="javascript:unhide('bn20tldr');">tl;dr</a>
	<div id="bn20bib" class="hidden"><div class="bibstyle"><br/>
@article{de2020batch, <br/>
  title={Batch normalization biases residual blocks towards the identity function in deep networks}, <br/>
  author={De, Soham and Smith, Sam}, <br/>
  journal={Advances in Neural Information Processing Systems}, <br/>
  volume={33}, <br/>
  year={2020} <br/>
} <br/>
	</div></div>
	<div id="bn20tldr" class="hidden"><br/>
We show that batch normalization biases residual blocks towards the identity function in deep networks early in training. This dramatically increases the largest trainable depth. We can recover this benefit with a simple change to the initialization scheme.
	</div>
	<br/><br/>

<!--
	<li>
	<span class="title">On the Generalization Benefit of Noise in Stochastic Gradient Descent</span><br/> 
	Samuel L Smith, Erich Elsen, <b>Soham De</b><br/>
	ICML 2020<br/>
	<a href="https://arxiv.org/pdf/2006.15081.pdf" target="_blank">paper</a> /
	<a href="javascript:unhide('sgdgen20bib');">bibtex</a> /
	<a href="javascript:unhide('sgdgen20tldr');">tl;dr</a>
	<div id="sgdgen20bib" class="hidden"><div class="bibstyle"><br/>
@article{smith2020generalization, <br/>
  title={On the generalization benefit of noise in stochastic gradient descent}, <br/>
  author={Smith, Samuel L and Elsen, Erich and De, Soham}, <br/>
  journal={arXiv preprint arXiv:2006.15081}, <br/>
  year={2020} <br/>
} <br/>
	</div></div>
	<div id="sgdgen20tldr" class="hidden"><br/>
While it has long been argued that minibatch SGD can generalize better than large batch gradient descent in neural networks, recent papers have questioned this claim. Through carefully designed experiments and rigorous hyperparameter sweeps, we verify that SGD with small batch sizes can outperform large batches on the test set.
	</div>
	<br/><br/>
-->

	<li>
	<span class="title">The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent</span><br/> 
	Karthik A. Sankararaman*, <b>Soham De*</b>, Zheng Xu, W. Ronny Huang, Tom Goldstein<br/>
	ICML 2020<br/>
	<a href="https://arxiv.org/pdf/1904.06963.pdf" target="_blank">paper</a> /
	<a href="javascript:unhide('gc19bib');">bibtex</a> /
	<a href="javascript:unhide('gc19tldr');">tl;dr</a>
	<div id="gc19bib" class="hidden"><div class="bibstyle"><br/>
@article{sankararaman2019impact, <br/>
  title={The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent}, <br/>
  author={Sankararaman, Karthik A and De, Soham and Xu, Zheng and Huang, W Ronny and Goldstein, Tom}, <br/>
  journal={arXiv preprint arXiv:1904.06963}, <br/>
  year={2019} <br/>
} <br/>
	</div></div>
	<div id="gc19tldr" class="hidden"><br/>
At standard Gaussian initializations, we prove that increased layer width improves and increased network depth hurts trainability of neural networks. Orthogonal initializations make the early training dynamics independent of depth, but is applicable only for linear or tanh networks. Finally, we show that the combination of batch normalization and skip connections enable us to train very deep networks in practice.
	</div>
	<br/><br/>

<!-- 	<li>
	<span class="title">Adversarial robustness through local linearization</span><br/> 
	Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Alhussein Fawzi, <b>Soham De</b>, Robert Stanforth, Pushmeet Kohli<br/>
	NeurIPS 2019<br/>
	<a href="https://arxiv.org/pdf/1907.02610.pdf" target="_blank">paper</a> /
	<a href="javascript:unhide('llr19bib');">bibtex</a> /
	<a href="javascript:unhide('llr19tldr');">tl;dr</a>
	<div id="llr19bib" class="hidden"><div class="bibstyle"><br/>
@article{qin2019adversarial, <br/>
  title={Adversarial Robustness through Local Linearization}, <br/>
  author={Qin, Chongli and Martens, James and Gowal, Sven and Krishnan, Dilip and Fawzi, Alhussein and De, Soham and Stanforth, Robert and Kohli, Pushmeet}, <br/>
  journal={arXiv preprint arXiv:1907.02610}, <br/>
  year={2019} <br/>
} <br/>
	</div></div>
	<div id="llr19tldr" class="hidden"><br/>
Adversarial training is an effective but computationally costly method for training neural nets that are robust against adversarial perturbations. We introduce a regularizer that achieves state-of-the-art adversarial accuracy results on CIFAR-10 and ImageNet classifiers, while being significantly faster than adversarial training.
	</div>
	<br/><br/> -->

	<li>
	<span class="title">Training Quantized Nets: A Deeper Understanding</a></span><br/> 
	Hao Li*, <b>Soham De*</b>, Zheng Xu, Christoph Studer, Hanan Samet, Tom Goldstein<br/>
	NeurIPS 2017<br/>
	<!-- Neural Information Processing Systems (NIPS) 2017<br/> -->
	<a href="https://arxiv.org/pdf/1706.02379.pdf" target="_blank">paper</a> /
	<a href="docs/nips17_poster.pdf" target="_blank">poster</a> /
	<a href="javascript:unhide('nips17bib');">bibtex</a> /
	<a href="javascript:unhide('nips17tldr');">tl;dr</a>
	<div id="nips17bib" class="hidden"><div class="bibstyle"><br/>
@inproceedings{li2017training, <br/>
  title={Training quantized nets: A deeper understanding}, <br/>
  author={Li, Hao and De, Soham and Xu, Zheng and Studer, Christoph and Samet, Hanan and Goldstein, Tom}, <br/>
  booktitle={Advances in Neural Information Processing Systems}, <br/>
  pages={5811--5821}, <br/>
  year={2017} <br/>
} <br/>
	</div></div>
	<div id="nips17tldr" class="hidden"><br/>
Neural net parameters can often be compressed down to just one single bit without a significant loss in network performance, yielding a huge reduction in model size and computational workload. We develop a theory of quantized nets, and explain the performance of algorithms for weight quantization.
	</div>
	<br/><br/>

<!--
	<li>
	<span class="title">Automated Inference with Adaptive Batches</a></span><br/> 
	<b>Soham De</b>, Abhay Yadav, David Jacobs, Tom Goldstein<br/>
	AISTATS 2017<br/>
	<a href="docs/aistats17_paper.pdf" target="_blank">paper</a> /
	<a href="docs/aistats17_slides.pdf" target="_blank">slides</a> /
	<a href="javascript:unhide('aistats17bib');">bibtex</a> /
	<a href="javascript:unhide('aistats17tldr');">tl;dr</a>
	<div id="aistats17bib" class="hidden"><div class="bibstyle"><br/>
@inproceedings{de2017automated, <br/>
  title={Automated inference with adaptive batches}, <br/>
  author={De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom}, <br/>
  booktitle={Artificial Intelligence and Statistics}, <br/>
  pages={1504--1513}, <br/>
  year={2017} <br/>
} <br/>
	</div></div>
	<div id="aistats17tldr" class="hidden"><br/>
We propose Big Batch SGD for automatically growing batch sizes by controlling the signal-to-noise ratio during SGD training. We show that increasing the batch size during training can generalize as well as small batches on neural networks, and allows for SGD to be fully automated using adaptive step size methods.
	</div>
	<br/><br/>
-->

<!-- 	<li>
	<span class="title">The inevitability of ethnocentrism revisited: ethnocentrism diminishes as mobility increases</span><br/> 
	<b>Soham De</b>, Michele Gelfand, Dana Nau, Patrick Roos<br/>
	Scientific Reports 2015<br/>
	<a href="http://www.nature.com/articles/srep17963" target="_blank">paper</a> /
	<a href="https://cmns.umd.edu/news-events/features/3350" target="_blank">press</a> /
	<a href="javascript:unhide('scirep15bib');">bibtex</a> /
	<a href="javascript:unhide('scirep15tldr');">tl;dr</a>
	<div id="scirep15bib" class="hidden"><div class="bibstyle"><br/>
@article{de2015inevitability, <br/>
  title={The inevitability of ethnocentrism revisited: Ethnocentrism diminishes as mobility increases}, <br/>
  author={De, Soham and Gelfand, Michele J and Nau, Dana and Roos, Patrick}, <br/>
  journal={Scientific reports}, <br/>
  volume={5}, <br/>
  pages={17963}, <br/>
  year={2015}, <br/>
  publisher={Nature Publishing Group} <br/>
} <br/>
	</div></div>
	<div id="scirep15tldr" class="hidden"><br/>
Advances over the centuries have greatly increased the degree to which humans change physical locations. Using an evolutionary game theoretical model and archival data, we show that in highly mobile societies, one’s choice of action is more likely to depend on what individual one is interacting with, rather than the group to which the individual belongs.
	</div>
	<br/><br/> -->
</ul>

<hr/>
Last Updated: June 23, 2022

</body>
</html>
